\documentclass{report}
\usepackage{graphicx}
\begin{document}
\title{Eigenvalue Equation}
\author{V JANARTHANAN VASANTH (JV060489)}
\date{June 2023}
\maketitle
\renewcommand{\thesection}{\arabic{section}}
\renewcommand{\thesubsection}{\arabic{section}.\arabic{subsection}}
\section{Eigenvalue Equation}
\centerline{A$\lambda$ = v$\lambda$}\footnote{I looked up this from my MA1102 course.}
A complex number $\lambda$ is called an eigenvalue of A iff there exists a
non-zero vector v $\in$ $C^(nX1)$ such that   A$\lambda$ = v$\lambda$. Such a vector v is called an eigenvector
of A for (or, associated with, or, corresponding to) the eigenvalue $\lambda$


\vspace{\baselineskip}
\subsection{Explanation}

When a matrix v acts on a vector A, it transforms v into a new vector. In some special cases, the resulting vector is simply a scaled version of the original vector, i.e., v is parallel to Av. The scalar value $\lambda$ represents the scale factor, and v is referred to as the eigenvector corresponding to the eigenvalue $\lambda$.


\vspace{\baselineskip}
\subsection{History}
Eigenvalues originated from the study of quadratic forms and differential equations. In the 18th century, Euler explored rotational motion and identified principal axes. Lagrange linked these axes to eigenvectors of the inertia matrix. Cauchy classified quadric surfaces and introduced the term "racine caract√©ristique" (now eigenvalue). Fourier, Sturm, and Hermite expanded on these concepts.

Brioschi and Clebsch studied orthogonal and skew-symmetric matrices. Weierstrass addressed matrix stability. Liouville and Schwarz contributed to Sturm-Liouville theory. Hilbert introduced the term "eigen" for eigenvalues and eigenvectors. The power method, the first numerical algorithm for computing eigenvalues, was published by von Mises in 1929. The QR algorithm was independently proposed by Francis and Kublanovskaya in 1961.














\subsection{Application}
Eigenvectors and eigenvalues have several significant applications. They play a crucial role in diagonalization, eigenvalue decomposition, and spectral decomposition of matrices, which can simplify various calculations and provide insights into the behavior of linear systems. Eigenvalues and eigenvectors are also used in principal component analysis (PCA), solving differential equations, and many other areas of mathematics and physics.





\end{document}
